{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daff8115",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Author: Ivan Gvozdanovic\n",
    "Date: 10/02/2024\n",
    "Description:\n",
    "    GAE A2C algorithm for solving the TSP using Markov Basis.\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f43b1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Good reading material: \n",
    "\n",
    "    https://github.com/tsmatz/reinforcement-learning-tutorials/blob/master/03-actor-critic.ipynb\n",
    "    https://github.com/Lucasc-99/Actor-Critic/blob/master/src/a2c.py\n",
    "    https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py\n",
    "    https://github.com/Francesco-Sovrano/Framework-for-Actor-Critic-deep-reinforcement-learning-algorithms\n",
    "    https://github.com/tensorflow/docs/blob/master/site/en/tutorials/reinforcement_learning/actor_critic.ipynb\n",
    "    \n",
    "    https://www.reddit.com/r/reinforcementlearning/comments/aux7a5/question_about_nstep_learning_with_dqn/\n",
    "    https://datascience.stackexchange.com/questions/46245/off-policy-n-step-learning-with-dqn/46260#46260\n",
    "    https://people.cs.umass.edu/~barto/courses/cs687/Chapter%207.pdf\n",
    "    https://arxiv.org/pdf/1606.02647\n",
    "    https://arxiv.org/abs/1901.07510\n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9de4df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "   Idea:\n",
    "       2) Have multiple agents running parallel and then combine the policies in a affine combination to get a random trajectory. \n",
    "''';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d143edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import networkx as nx\n",
    "import itertools\n",
    "from collections import deque\n",
    "import time as Time\n",
    "import torch\n",
    "from collections import namedtuple\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "from DeepFiberSamplingENV import PolytopeENV as Env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "361e0594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_functions import reward_cost, calculate_reward1, calculate_reward2, calculate_reward3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c1c4aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution: \n",
      " 10 [0 1 1 0 0 0 1 0 0 0]\n",
      "Sufficient statistic: \n",
      " [2. 1. 1. 1. 1.]\n",
      "Number of actions is 5\n",
      "{0: array([0, 1, 1, 0, 0, 0, 1, 0, 0, 0])}\n",
      "[ 0  1 -1  0 -1  1  0  0  0  0]\n",
      "[ 0  1  0 -1 -1  0  1  0  0  0]\n",
      "[ 1  0 -1  0 -1  0  0  1  0  0]\n",
      "[ 1  0  0 -1 -1  0  0  0  1  0]\n",
      "[ 1  1 -1 -1 -1  0  0  0  0  1]\n"
     ]
    }
   ],
   "source": [
    "from helper_functions import create_fiber_sampling_erdos_renyi_graph, \\\n",
    "                             extract_distance_matrix, \\\n",
    "                             create_real_data_graph, \\\n",
    "                             create_real_data_initial_sol,\\\n",
    "                             moving_average, \\\n",
    "                             create_state_graph, \\\n",
    "                             permute_moves\n",
    "\n",
    "\n",
    "# path_initial = os.getcwd() + os.sep + 'Real Data' + os.sep + 'LargestComponent' + os.sep + 'largeComponentMtx.txt'\n",
    "path_initial = os.getcwd() + os.sep + 'Real Data' + os.sep + 'MediumComponent' + os.sep + 'nextComponentMtx.txt'\n",
    "\n",
    "initial_states = {} # dictionary holding the initial states.\n",
    "patches = 1\n",
    "node_num = 5\n",
    "p = 0.5\n",
    "graph_num = 1\n",
    "\n",
    "#Pick the file to the problem:\n",
    "file = 'A2C_Fiber_Sampling_'\n",
    "\n",
    "\n",
    "available_actions, initial_states = create_fiber_sampling_erdos_renyi_graph(file, initial_states, node_num, p, graph_num)\n",
    "# initial_states, available_actions, node_num = create_real_data_graph(path_initial) # works for smaller problems where we can compute lattice.\n",
    "\n",
    "\n",
    "# # print(available_actions)\n",
    "print(initial_states)\n",
    "for m in range(len(available_actions)):\n",
    "    if m < 76:\n",
    "        print(available_actions[m])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ee7ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.8359588020779369\n",
      "0.7116851017915987\n",
      "[1, 1, 1, 1, 1]\n",
      "5\n",
      "[[-1, 0, 1], [-1, 0, 1], [-1, 0, 1], [-1, 0, 1], [-1, 0, 1]]\n"
     ]
    }
   ],
   "source": [
    "save_data = True  # save Q table data and cost vector data.\n",
    "save_plots = False  # save the plots\n",
    "save_data_rate = 20\n",
    "\n",
    "\n",
    "\n",
    "# Example usage for running episodes\n",
    "num_episodes = 2\n",
    "max_path_length = 20\n",
    "\n",
    "\n",
    "\n",
    "n_step = max_path_length\n",
    "sheduler_lr_update = 10 # every 10 trainig periods we modify the step size\n",
    "gamma = 0.9\n",
    "lam = 0.5\n",
    "discount_factor = gamma\n",
    "entropy_param = 0.5\n",
    "\n",
    "actor_target_lr = 0.00001\n",
    "critic_target_lr = 0.000001  # Set a lower target for faster convergence\n",
    "\n",
    "actor_lr = 0.00006\n",
    "critic_lr = 0.00003\n",
    "step_size = num_episodes//sheduler_lr_update\n",
    "lr_actor_gamma = (actor_target_lr / actor_lr) ** (1 / sheduler_lr_update)\n",
    "lr_critic_gamma = (critic_target_lr / critic_lr) ** (1 / sheduler_lr_update)\n",
    "print(step_size)\n",
    "print(lr_actor_gamma)\n",
    "print(lr_critic_gamma)\n",
    "\n",
    "\n",
    "\n",
    "lb = -1\n",
    "ub = 2\n",
    "\n",
    "\n",
    "\n",
    "mask_size = 5\n",
    "mask_rate = 50\n",
    "mask_action_size = [len(available_actions)//mask_size for i in range(mask_size)]\n",
    "mask_action_size[-1] += len(available_actions)%mask_size # add the remained if not divisible.\n",
    "mask_range = mask_action_size[0]\n",
    "print(mask_action_size)\n",
    "action_space_values = [ [i+lb for i in range(ub-lb)] for j in range(len(available_actions)) ]\n",
    "action_space_size = [ub-lb for i in range(len(available_actions))]\n",
    "print(len(action_space_size))\n",
    "print(action_space_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a119073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerPolicy(\n",
      "  (embedding): Linear(in_features=10, out_features=16, bias=True)\n",
      "  (positional_encoding): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-1): 2 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (action_heads): ModuleList(\n",
      "    (0-4): 5 x Linear(in_features=16, out_features=3, bias=True)\n",
      "  )\n",
      "  (mask_heads): ModuleList(\n",
      "    (0-4): 5 x Linear(in_features=16, out_features=1, bias=True)\n",
      "  )\n",
      "  (value_head): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gvozd\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from A2C import Policy, \\\n",
    "                TransformerPolicy, \\\n",
    "                select_action, \\\n",
    "                select_action_transformer, \\\n",
    "                run_n_step_with_gae, \\\n",
    "                select_best_action, \\\n",
    "                select_best_action_transformer, \\\n",
    "                freeze_parameters, \\\n",
    "                generate_mask,\\\n",
    "                construct_stochastic_policy\n",
    "\n",
    "\n",
    "feature_net_arch = [len(initial_states[0]), 56, 28, 12, 28]\n",
    "# model = Policy(feature_net_arch, len(initial_states[0]), len(action_space_values))\n",
    "model = TransformerPolicy(len(initial_states[0]), action_space_size, mask_action_size, mask_rate)\n",
    "print(model)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.transformer_encoder.parameters()},   # Shared feature extractor\n",
    "    {'params': model.action_heads.parameters(), 'lr': actor_lr},  # Actor-specific parameters\n",
    "    {'params': model.value_head.parameters(), 'lr': critic_lr} # Critic-specific parameters\n",
    "])\n",
    "\n",
    "scheduler_actor = StepLR(optimizer, step_size=step_size, gamma=lr_actor_gamma)\n",
    "scheduler_critic = StepLR(optimizer, step_size=step_size, gamma=lr_critic_gamma)\n",
    "\n",
    "\n",
    "# actor_params = list(model.action_head.parameters()) + list(model.feature_net.parameters())  # Actor network params\n",
    "# critic_params = list(model.value_head.parameters())  # Critic network params\n",
    "# actor_optimizer = torch.optim.Adam(actor_params, lr=actor_lr)  # Learning rate for actor\n",
    "# critic_optimizer = torch.optim.Adam(critic_params, lr=critic_lr)  # Learning rate for critic\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "SavedAction = namedtuple('SavedAction', ['log_probs', 'value', 'probs', 'mask_log_probs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2975330a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert dictionary values to a list of arrays\n",
    "# visited_states = [np.array(initial_states[0])]\n",
    "# visited_states = np.stack(visited_states)\n",
    "\n",
    "# #Initialize the environment.\n",
    "# env = Env(initial_states, # initial_state\n",
    "#          num_episodes, # total_episodes\n",
    "#          50, # show_path_num\n",
    "#          visited_states,  # visited_states\n",
    "#          available_actions, # basis_moves\n",
    "#          node_num, # node_num\n",
    "#          0, # P\n",
    "#          lb, #lb\n",
    "#          )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a426bc-08be-4c76-9a33-5db094351390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env_params, model_class, model_args, optimizer_args, scheduler_args, agent_id, num_episodes, save_data_rate, result_queue):\n",
    "    \"\"\"\n",
    "    Function to run multiple episodes for a single agent in a separate process.\n",
    "    Each agent has its own model, and the model is saved independently.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the environment\n",
    "    env = Env(*env_params)\n",
    "\n",
    "\n",
    "    \n",
    "    # Initialize a separate model instance for this agent\n",
    "    model = model_class(**model_args)\n",
    "\n",
    "    # Initialize optimizer and schedulers\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.transformer_encoder.parameters()},   # Shared feature extractor\n",
    "        {'params': model.action_heads.parameters(), 'lr': optimizer_args['actor_lr']},  # Actor-specific parameters\n",
    "        {'params': model.value_head.parameters(), 'lr': optimizer_args['critic_lr']}  # Critic-specific parameters\n",
    "    ])\n",
    "    scheduler_actor = StepLR(optimizer, **scheduler_args['actor'])\n",
    "    scheduler_critic = StepLR(optimizer, **scheduler_args['critic'])\n",
    "\n",
    "    # Initialize tracking variables\n",
    "    episode_reward_list = []\n",
    "    cumm_running_reward = 0\n",
    "    cumm_running_reward_list = []\n",
    "    actor_lr_list = []\n",
    "    critic_lr_list = []\n",
    "    robins_monro_condition = []\n",
    "    iteration = 0\n",
    "\n",
    "    # Run multiple episodes\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Reset environment and episode reward\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        for t in range(max_path_length):\n",
    "            # Select action from policy\n",
    "            action = select_action_transformer(model, state, SavedAction, action_space_values, mask_range, iteration, False, False)\n",
    "\n",
    "            # Take the action\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            model.rewards.append(reward)\n",
    "            ep_reward += (discount_factor**t) * reward\n",
    "            cumm_running_reward += reward\n",
    "            cumm_running_reward_list.append(reward)\n",
    "\n",
    "            # Perform backpropagation step\n",
    "            actor_lr, critic_lr = run_n_step_with_gae(\n",
    "                model, n_step, gamma, lam, optimizer, scheduler_actor, scheduler_critic, lr_actor_gamma, lr_critic_gamma, entropy_param, done\n",
    "            )\n",
    "            \n",
    "            if actor_lr is not None and critic_lr is not None:\n",
    "                actor_lr_list.append(actor_lr)\n",
    "                critic_lr_list.append(critic_lr)\n",
    "                robins_monro_condition.append(critic_lr / actor_lr)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        # Save rewards and other metrics\n",
    "        episode_reward_list.append(ep_reward)\n",
    "\n",
    "        # Save the model periodically for this agent\n",
    "        if (i_episode + 1) % save_data_rate == 0:\n",
    "            os.makedirs(f'Models/Agent_{agent_id}', exist_ok=True)\n",
    "            torch.save(model.state_dict(), f'Models/Agent_{agent_id}/policy_model_EP_{i_episode + 1}.pth')\n",
    "            print(f'Agent {agent_id}: Saved model at episode {i_episode + 1}.')\n",
    "\n",
    "    # Send results to the main process\n",
    "    result_queue.put({\n",
    "        'agent_id': agent_id,\n",
    "        'episode_reward_list': episode_reward_list,\n",
    "        'cumm_running_reward_list': cumm_running_reward_list,\n",
    "        'actor_lr_list': actor_lr_list,\n",
    "        'critic_lr_list': critic_lr_list,\n",
    "        'robins_monro_condition': robins_monro_condition\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61c040f4-8666-4d15-ae6e-f8920553724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_independent_agents(env_params, model_class, model_args, optimizer_args, scheduler_args, num_agents, num_episodes, save_data_rate):\n",
    "    \"\"\"\n",
    "    Runs multiple independent agents in parallel, each with its own policy model.\n",
    "    \"\"\"\n",
    "    mp.get_context(\"spawn\")\n",
    "    result_queue = mp.Queue()\n",
    "\n",
    "    # Start each agent in its own process\n",
    "    processes = []\n",
    "    for agent_id in range(num_agents):\n",
    "        p = mp.Process(target=run_episode, args=(env_params, model_class, model_args, optimizer_args, scheduler_args, agent_id, num_episodes, save_data_rate, result_queue))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "        print(f'Agent id {agent_id}')\n",
    "\n",
    "    # Collect results from each agent\n",
    "    all_agent_results = {}\n",
    "    for _ in range(num_agents):\n",
    "        result = result_queue.get()\n",
    "        agent_id = result['agent_id']\n",
    "        all_agent_results[agent_id] = result\n",
    "        print(\"HEllo\")\n",
    "\n",
    "    # Wait for all processes to finish\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    return all_agent_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48887c8d-4f98-402d-92a2-3a2362db339f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent id 0\n",
      "Agent id 1\n"
     ]
    }
   ],
   "source": [
    "# Define environment and model parameters\n",
    "visited_states = [np.array(initial_states[0])]\n",
    "visited_states = np.stack(visited_states)\n",
    "env_params = (\n",
    "    initial_states, num_episodes, 50, visited_states, available_actions, node_num, 0, lb\n",
    ")\n",
    "\n",
    "\n",
    "model_class = TransformerPolicy  # Class, not an instance\n",
    "model_args = {\n",
    "    \"input_size\": len(initial_states[0]),\n",
    "    \"action_space\": action_space_size,\n",
    "    \"mask_action_space\": mask_action_size,\n",
    "    \"mask_rate\": mask_rate\n",
    "}\n",
    "\n",
    "# Optimizer and scheduler parameters\n",
    "optimizer_args = {\n",
    "    'actor_lr': actor_lr,\n",
    "    'critic_lr': critic_lr\n",
    "}\n",
    "scheduler_args = {\n",
    "    'actor': {'step_size': step_size, 'gamma': lr_actor_gamma},\n",
    "    'critic': {'step_size': step_size, 'gamma': lr_critic_gamma}\n",
    "}\n",
    "\n",
    "# Run N independent agents\n",
    "num_agents = 2\n",
    "results = run_independent_agents(env_params, model_class, model_args, optimizer_args, scheduler_args, num_agents, num_episodes, save_data_rate)\n",
    "print(\"Training results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b14df30",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# cumm_running_reward = 0\n",
    "# episode_reward_list = []\n",
    "# cumm_running_reward_list = []\n",
    "# loss_list = []\n",
    "# actor_lr_list = []\n",
    "# critic_lr_list = []\n",
    "# robins_monro_condition = []\n",
    "\n",
    "\n",
    "# start_time = Time.time()\n",
    "# iteration = 0\n",
    "\n",
    "\n",
    "# # run infinitely many episodes\n",
    "# for i_episode in range(num_episodes):\n",
    "\n",
    "#     # reset environment and episode reward\n",
    "#     state = env.reset()\n",
    "#     ep_reward = 0\n",
    "\n",
    "#     # for each episode, only run 9999 steps so that we don't\n",
    "#     # infinite loop while learning\n",
    "#     for t in range(max_path_length):\n",
    "\n",
    "\n",
    "#         # select action from policy\n",
    "# #         action = select_action(model, state, SavedAction, action_space_values, False)\n",
    "#         action = select_action_transformer(model, state, SavedAction, action_space_values, mask_range, iteration, False, False)\n",
    "        \n",
    "        \n",
    "#         # take the action\n",
    "#         state, reward, done, _ = env.step(action)\n",
    "        \n",
    "#         model.rewards.append(reward)\n",
    "#         ep_reward += (discount_factor**t)*reward\n",
    "#         cumm_running_reward += reward\n",
    "#         cumm_running_reward_list.append(reward)\n",
    "#         # perform backprop\n",
    "#         actor_lr,critic_lr = run_n_step_with_gae(model, n_step, gamma, lam, optimizer, scheduler_actor, scheduler_critic, lr_actor_gamma, lr_critic_gamma, entropy_param, done)\n",
    "#         if actor_lr != None and critic_lr != None:\n",
    "#             actor_lr_list.append(actor_lr)\n",
    "#             critic_lr_list.append(critic_lr)\n",
    "#             robins_monro_condition.append(critic_lr/actor_lr)\n",
    "#         if done:\n",
    "#             break\n",
    "        \n",
    "#         iteration += 1\n",
    "        \n",
    "#     if (i_episode + 1) % save_data_rate == 0:\n",
    "#         torch.save(model.state_dict(), 'Models/policy_model_Node#_' + str(node_num) + \"_EP_\" + str(num_episodes) +  '.pth')\n",
    "#         end_time = Time.time()\n",
    "#         print(f'It took {(end_time-start_time)/60} minutes to run {i_episode} episodes.')\n",
    "        \n",
    "#     episode_reward_list.append(ep_reward)\n",
    "    \n",
    "# torch.save(model.state_dict(), 'Models/policy_model_Node#_' + str(node_num) + \"_EP_\" + str(num_episodes) +  '.pth')\n",
    "\n",
    "\n",
    "# end_time = Time.time()\n",
    "# print(f'It took {(end_time-start_time)/60} minutes to run {num_episodes} episodes.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1adb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cum_reward_x_axis = [i for i in range(len(cumm_running_reward_list))]\n",
    "ep_reward_x_axis = [i for i in range(len(episode_reward_list))]\n",
    "loss_x_axis = [i for i in range(len(loss_list))]\n",
    "actor_lr_x_axis = [i for i in range(len(actor_lr_list))]\n",
    "critic_lr_x_axis = [i for i in range(len(critic_lr_list))]\n",
    "\n",
    "smoothed_rewards = moving_average(episode_reward_list, 10)\n",
    "smoothed_rewards_x_axis = [i for i in range(len(smoothed_rewards))]\n",
    "\n",
    "plt.plot(smoothed_rewards_x_axis, smoothed_rewards)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Moving Average\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(ep_reward_x_axis, episode_reward_list)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Episodic Rewards\")\n",
    "plt.show()\n",
    "plt.plot(cum_reward_x_axis, cumm_running_reward_list)\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.ylabel(\"Cumulative Reward\")\n",
    "plt.show()\n",
    "plt.plot(actor_lr_x_axis, actor_lr_list)\n",
    "plt.title(\"Actor loss\")\n",
    "plt.show()\n",
    "plt.plot(critic_lr_x_axis, critic_lr_list)\n",
    "plt.title(\"Critic loss\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot(critic_lr_x_axis,robins_monro_condition)\n",
    "# plt.xticks(ticks=custom_x_values, labels=custom_x_labels, rotation=45, ha='right', size='small')\n",
    "plt.grid(visible=True)\n",
    "plt.title(\"Robins-Monro convergence condition\")\n",
    "plt.ylabel(\"(Actor LR)/(Critic LR) \")\n",
    "plt.xlabel(\"Episodes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b2ec2f-6514-4cbe-9f13-3146a766d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_actions(state, actions, action_probabilities):\n",
    "    filtered_actions = []\n",
    "    filtered_actions_probs = []\n",
    "    for a in range(len(actions)):\n",
    "        next_state = np.add(state, actions[a])\n",
    "        if all(coord >= 0 for coord in next_state):\n",
    "            filtered_actions.append(actions[a])\n",
    "            print(\"Possible action: \", actions[a], \" with probability \", action_probabilities[a], \" ordered at position \", a)\n",
    "            filtered_actions_probs.append(action_probabilities[a])\n",
    "    return filtered_actions,filtered_actions_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model \n",
    "feature_net_arch = [len(initial_states[0]), 56, 28, 12, 28]\n",
    "# model = Policy(feature_net_arch, len(initial_states[0]), len(action_space_values))\n",
    "model = TransformerPolicy(len(initial_states[0]), action_space_size, mask_action_size, mask_rate)\n",
    "model.load_state_dict(torch.load('Models/policy_model_Node#_' + str(node_num) + \"_EP_\" + str(num_episodes) +  '.pth')) #+ str(node_num)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "initial_states = {0: np.array([0, 1, 0, 0, 1, 0, 1, 0, 0, 1])}\n",
    "visited_states = [initial_states[0]]\n",
    "min_rewards = []\n",
    "sim_number = 1\n",
    "optimum_reached = 0\n",
    "max_path_length = 10\n",
    "\n",
    "for sim in range(sim_number):\n",
    "\n",
    "    print(\"########################################################################\")\n",
    "    print(\"########################################################################\")\n",
    "    print(\"########################################################################\")\n",
    "    print(\"########################################################################\")\n",
    "    print(\"########################################################################\")\n",
    "    \n",
    "    # Convert dictionary values to a list of arrays\n",
    "    # visited_states = [np.array(initial_states[0])]\n",
    "    visited_states = np.stack(visited_states)\n",
    "\n",
    "    #Initialize the environment.\n",
    "    env = Env(initial_states, # initial_state\n",
    "             num_episodes, # total_episodes\n",
    "             50, # show_path_num\n",
    "             visited_states,  # visited_states\n",
    "             available_actions, # basis_moves\n",
    "             node_num, # node_num\n",
    "             0, # P\n",
    "             lb, #lb\n",
    "             )\n",
    "\n",
    "    # reset environment and episode reward\n",
    "    state = env.reset()\n",
    " \n",
    "    # for each episode, only run 9999 steps so that we don't\n",
    "    # infinite loop while learning\n",
    "    for t in range(max_path_length):\n",
    "\n",
    "#         action = select_best_action_transformer(model, state, action_space_values, t, mask_range)\n",
    "        # action = select_action_transformer(model, state, SavedAction, action_space_values, mask_range, t, None, True)\n",
    "        actions, action_probabilities = construct_stochastic_policy(model, state, SavedAction, action_space_values, mask_range, t, False, 0.01)\n",
    "        action_choices = []\n",
    "        for a in actions:\n",
    "            action_rounded = np.array(np.round(a), dtype=int)\n",
    "            all_actions = [np.multiply(action_rounded[i], available_actions[i]) for i in range(len(action_rounded))]\n",
    "            all_actions = np.stack(all_actions)\n",
    "            action = np.sum(all_actions, 0)\n",
    "            action_choices.append(action)\n",
    "            \n",
    "        action_choices, filtered_actions_probs = filter_actions(state, action_choices, action_probabilities)\n",
    "        if len(action_choices) > 1:\n",
    "            random_action = np.random.randint(0,len(action_choices))\n",
    "        else:\n",
    "            random_action = 0\n",
    "        print(random_action)\n",
    "        action = action_choices[random_action]\n",
    "        print(\"Action: \",action, \" with probability \", filtered_actions_probs[random_action])\n",
    "        next_state = np.add(state, action)\n",
    "        print(\"Next state: \", next_state)\n",
    "        if all(coord >= 0 for coord in next_state):\n",
    "            if next_state.tolist() not in visited_states.tolist():\n",
    "                visited_states = np.concatenate((visited_states,[next_state]),axis=0)\n",
    "            state = next_state\n",
    "        print(\"#####################\")\n",
    "    \n",
    "    print(f'We discovered {visited_states.shape[0]} unique states using the optimal stochastic policy')\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a97882-6bde-4fe6-81fd-695e1b533f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = [1,2,3]\n",
    "print(np.argmax(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d8c396-896b-4fbe-985e-b192ec69db65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
